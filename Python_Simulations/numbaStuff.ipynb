{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numba import *\n",
      "import numpy as np\n",
      "@autojit\n",
      "def matrix_vector(M, v):\n",
      "    return np.sum(M * v, axis=1)\n",
      "\n",
      "@jit(restype=None, argtypes=[int64[:,:],int32[:]], backend='ast', target='cpu', nopython=False)\n",
      "def numbaDot(M,v):\n",
      "    return np.dot(M,v)\n",
      "\n",
      "M = np.arange(90).reshape(9, 10)\n",
      "v = np.arange(10)\n",
      "print (typeof(M))\n",
      "%timeit matrix_vector(M, v)\n",
      "%timeit np.dot(M, v)\n",
      "%timeit numbaDot(M, v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "int64[:, :]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 14.1 \u00b5s per loop\n",
        "1000000 loops, best of 3: 1.89 \u00b5s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1000000 loops, best of 3: 1.96 \u00b5s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numba import *\n",
      "import numpy as np\n",
      "\n",
      "mu = 0.1\n",
      "Lx, Ly = 101, 101\n",
      "N = 1000\n",
      "\n",
      "@autojit\n",
      "def diffuse_loops(iter_num):\n",
      "    u = np.zeros((Lx, Ly), dtype=np.float64)\n",
      "    temp_u = np.zeros_like(u)\n",
      "    temp_u[Lx / 2, Ly / 2] = 1000.0\n",
      "\n",
      "    for n in range(iter_num):\n",
      "        for i in range(1, Lx - 1):\n",
      "            for j in range(1, Ly - 1):\n",
      "                u[i, j] = mu * (temp_u[i + 1, j] + temp_u[i - 1, j] +\n",
      "                                temp_u[i, j + 1] + temp_u[i, j - 1] -\n",
      "                                4 * temp_u[i, j])\n",
      "\n",
      "        temp = u\n",
      "        u = temp_u\n",
      "        temp_u = temp\n",
      "\n",
      "    return u\n",
      "\n",
      "@autojit\n",
      "def diffuse_array_expressions(iter_num):\n",
      "    u = np.zeros((Lx, Ly), dtype=np.float64)\n",
      "    temp_u = np.zeros_like(u)\n",
      "    temp_u[Lx / 2, Ly / 2] = 1000.0\n",
      "\n",
      "    for i in range(iter_num):\n",
      "        u[1:-1, 1:-1] = mu * (temp_u[2:, 1:-1] + temp_u[:-2, 1:-1] +\n",
      "                              temp_u[1:-1, 2:] + temp_u[1:-1, :-2] -\n",
      "                              4 * temp_u[1:-1, 1:-1])\n",
      "\n",
      "        temp = u\n",
      "        u = temp_u\n",
      "        temp_u = temp\n",
      "\n",
      "    return u\n",
      "\n",
      "%timeit diffuse_loops(100)\n",
      "%timeit diffuse_array_expressions(100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 4.2 ms per loop\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 1.99 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@jit(restype=None, argtypes=[int32[:]], backend='ast', target='cpu', nopython=False)\n",
      "def square(a):\n",
      "    return a * a\n",
      "\n",
      "a = np.arange(10)\n",
      "%timeit square(a) # array([ 0,  1,  4,  9, 16, 25, 36, 49, 64, 81])\n",
      "%timeit a * a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1000000 loops, best of 3: 538 ns per loop\n",
        "1000000 loops, best of 3: 1.94 \u00b5s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numba import *\n",
      "import numpy as np\n",
      "import numba as nb\n",
      "rem = nb.declare_instruction(int32(int32, int32), 'srem')\n",
      "\n",
      "@jit(int32(int32, int32))\n",
      "def py_modulo(a, n):\n",
      "     r = rem(a, n)\n",
      "     if r != 0 and (r ^ n) < 0:\n",
      "         r += n\n",
      "     return r\n",
      "\n",
      "# Instructions and intrinsics works directly in Python\n",
      "print rem(5, 2), rem(5, -2), rem(-5, 2), rem(-5, -2)\n",
      "\n",
      "\n",
      "# ... and are jitted in Numba functions\n",
      "print py_modulo(5, 2), py_modulo(5, -2), py_modulo(-5, 2), py_modulo(-5, -2)\n",
      "print np.mod(5,2), np.mod(5,-2), np.mod(-5,2), np.mod(-5,-2)\n",
      "%timeit np.mod(5,2)\n",
      "%timeit py_modulo(5,2)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 1 -1 -1\n",
        "1 -1 1 -1\n",
        "1 -1 1 -1\n",
        "100000 loops, best of 3: 3.52 \u00b5s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1000000 loops, best of 3: 175 ns per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    }
   ],
   "metadata": {}
  }
 ]
}